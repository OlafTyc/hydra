---
title: "Hydra report"
author:
- "Mattias  de Hollander (m.dehollander@nioo.knaw.nl)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
params:
    rmd: "report.Rmd"
output:
  html_document:
    highlight: tango
    number_sections: no
    theme: default
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: yes
    pandoc_args: ["--filter", "pandoc-fignos"]    
bibliography: hydra.bib
link-citations: true
---
# Hydra report for `r snakemake@config$project`

## Methods template

This analysis is performed by the Hydra pipeline [@hollander_nioo-knaw/hydra:_2017] implemented in Snakemake [@koster_snakemakescalable_2012]. All steps can be seen in figure @fig:description.
`r if (snakemake@params[["mergemethod"]] == "vsearch") "The fastq_mergeparis option of vsearch [@rognes_vsearch:_2016]" else if (snakemake@params[["mergemethod"]] == "pandaseq") "Pandaseq [@masella_pandaseq:_2012]"` has been used to merge paired-end reads. Sequences are converted to FASTA format and concatenated into a single file.
All reads are clustered into OTUs using `r if (snakemake@config$clustering == "usearch_smallmem") "the UPARSE strategy by dereplication, sorting by abundance with at least two sequences and clustering using the UCLUST smallmem algorithm [@edgar_search_2010]. These steps were performed with vsearch."`
Next, chimeric sequences were detected using the UCHIME algorithm in de-novo mode [@edgar_uchime_2011] implemented in VSEARCH. All reads before the dereplication step were mapped to OTUs using the usearch_global method implemented in VSEARCH to create an otutable and converted to BIOM-Format [@mcdonald_biological_2012].
Finally, taxonomic information for each OTU was added to the BIOM file by `r if (snakemake@config$classification == "silva") "aligning the sequences to the SILVA database (release 128) [@quast_silva_2013] using SINA [@pruesse_sina:_2012]" else if (snakemake@config$classification == "stampa") "aligning the sequences to the SILVA database (release 128) [@quast_silva_2013]. First the references sequences are trimmed with the forward (snakemake@config$forward_primer) and reverse primer (snakemake@config$forward_primer) using cutadapt [@martin_cutadapt_2011]. Next, sequences are aligned using the usearch_global method implemented in vsearch. At last, the last common ancestor taxonomy is determined for the top hits using a python script provided by STAMPA [@mahe_stampa:_2017]" else if (snakemake@config$classification == "rdp") "running the RDP Classifier re-trained on the UNITE database"`.

![](`r snakemake@input$workflow`){#fig:description}

OTU Clustering method: `r snakemake@config$clustering`

Taxonomic classification method: `r snakemake@config$classification`

Reads have been merged with `r snakemake@params[["mergemethod"]]`

### Input files

```{r kable, message=FALSE, warning=FALSE}
library(knitr)
kable(snakemake@config$data)
```

## Summary

```{r set_plot_theme,echo=F}
library(ggplot2)
plot.theme <- theme(axis.text.x=element_text(size=12,angle=0,colour="black"),
                    axis.text.y=element_text(size=12,angle=0,colour="black"),
                    axis.title=element_text(size=18,angle=0,colour="black"),
                    plot.title=element_text(size=18,angle=0,colour="black"),
                    text=element_text(margin=margin(1,1,1,1,"mm"),debug=F),
                    panel.grid.minor=element_line(color="grey95"),
                    strip.text=element_text(size=16),
                    panel.border=element_rect(linetype="solid",colour="grey")
                    )
```


## Plot
```{r readstat, message=FALSE, warning=FALSE}
library(plotly)
readstat = read.csv(snakemake@input$readstat)
p <- ggplot(readstat, aes(y=readstat$avg_len, x = seq(1, length(readstat$avg_len)))) + geom_point()

ggplotly(p)
```

## Sequence length reverse reads
```{r readstat_reverse, message=FALSE, warning=FALSE}
library(plotly)
readstat = read.csv(snakemake@input$readstat_reverse)
p <- ggplot(readstat, aes(y=readstat$avg_len, x = seq(1, length(readstat$avg_len)))) + geom_point()

ggplotly(p)
```

## phyloseq
```{r phyloseq, message=FALSE, warning=FALSE}
library(phyloseq)
import_biom(snakemake@input$biom)
```

## ampvis plot
```{r amvpis, message=FALSE, warning=FALSE}
library(phyloseq)
library(ampvis)
ds <- import_biom(snakemake@input$biom, parseFunction=parse_taxonomy_greengenes)
dsn <- transform_sample_counts(ds, function(x) x / sum(x) * 100)
amp_core(data = dsn,
         plot.type = "frequency",
         weight = T,
         scale.seq = 100)
```

## Downloads
<!--<script src="http://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>-->
<script>
function download(text, name, type) {
  var file = new Blob([text], {type: type})
  var a = $("#download")
  a[0].href = URL.createObjectURL(file)
  a[0].download = name
}
function dataURItoBlob(dataURI, callback) {
    // convert base64 to raw binary data held in a string
    // doesn't handle URLEncoded DataURIs - see SO answer #6850276 for code that does this
    var byteString = atob(dataURI.split(',')[1]);

    // separate out the mime component
    var mimeString = dataURI.split(',')[0].split(':')[1].split(';')[0]

    // write the bytes of the string to an ArrayBuffer
    var ab = new ArrayBuffer(byteString.length);
    var ia = new Uint8Array(ab);
    for (var i = 0; i < byteString.length; i++) {
        ia[i] = byteString.charCodeAt(i);
    }

    // write the ArrayBuffer to a blob, and you're done
    var bb = new Blob([ab]);
    return bb;
}
</script>

<a id="download" href="link">Download BIOM file</a>

<script>
dataURL = "`r base64enc::dataURI(file = snakemake@input$biom, mime = 'text/csv', encoding = 'base64')`"

download(dataURItoBlob(dataURL), "`r basename(snakemake@input$biom)`", 'text/plain')
</script>

<a download="readstat.csv" href="`r base64enc::dataURI(file = snakemake@input$readstat, mime = 'text/csv', encoding = 'base64')`">Readstat csv file</a>

<a download="report.Rmd" href="`r base64enc::dataURI(file = params$rmd, mime = 'text/rmd', encoding = 'base64')`">R Markdown source file (to produce this document)</a>

## References
